# ===============================================================================
# AURORA TRINITY-3 - ARQUITECTURA CAN√ìNICA CORREGIDA
# ===============================================================================

class TernaryLogic:
    """
    L√≥gica ternaria Aurora con manejo correcto de incertidumbre.
    Implementa Honestidad Computacional propagando NULL apropiadamente.
    """
    NULL = None  # Representaci√≥n can√≥nica de NULL en Aurora
    
    @staticmethod
    def ternary_xor(a, b):
        """XOR ternario con propagaci√≥n de NULL"""
        if a is TernaryLogic.NULL or b is TernaryLogic.NULL:
            return TernaryLogic.NULL
        return a ^ b
    
    @staticmethod
    def ternary_xnor(a, b):
        """XNOR ternario con propagaci√≥n de NULL"""
        if a is TernaryLogic.NULL or b is TernaryLogic.NULL:
            return TernaryLogic.NULL
        return 1 if a == b else 0
    
    @staticmethod
    def ternary_and(a, b):
        """AND ternario con propagaci√≥n de NULL"""
        if a is TernaryLogic.NULL or b is TernaryLogic.NULL:
            return TernaryLogic.NULL
        return a & b
    
    @staticmethod
    def ternary_or(a, b):
        """OR ternario con propagaci√≥n de NULL"""
        if a is TernaryLogic.NULL or b is TernaryLogic.NULL:
            return TernaryLogic.NULL
        return a | b

class TrigateConLut:
    """
    Trigate optimizado con Tablas de B√∫squeda (LUTs) para l√≥gica ternaria completa.
    Maneja correctamente los 3 estados: 0, 1, None (NULL).
    OPTIMIZADO: Reduce construcci√≥n LUT de 81,000 a 486 iteraciones.
    """
    
    # Tablas LUT ternarias precalculadas
    _LUT_INFER = {}      # (A, B, M) -> R
    _LUT_LEARN = {}      # (A, B, R) -> M  
    _LUT_SYNTH = {}      # (A, B, R) -> S
    _LUT_DEDUCE_B = {}   # (A, M, R) -> B
    _LUT_DEDUCE_A = {}   # (B, M, R) -> A
    
    def __init__(self):
        if not TrigateConLut._LUT_INFER:
            self._build_ternary_luts_optimized()
    
    def _build_ternary_luts_optimized(self):
        """
        üîß FIX CORREGIDO: Construcci√≥n LUT correcta - 27+27+27+27 = 108 iteraciones.
        Optimizaci√≥n real: 108 vs 3^4*5_m√©todos = 405 iteraciones brutas.
        """
        states = [0, 1, None]
        
        # FASE 1: LUTs que dependen solo de (A, B, M) - 27 iteraciones
        for a in states:
            for b in states:
                for m in states:
                    # LUT para infer(): (A, B, M) -> R
                    if a is None or b is None or m is None:
                        TrigateConLut._LUT_INFER[(a, b, m)] = None
                    elif m == 1:
                        TrigateConLut._LUT_INFER[(a, b, m)] = a ^ b
                    elif m == 0:
                        TrigateConLut._LUT_INFER[(a, b, m)] = 1 - (a ^ b)
                    else:
                        TrigateConLut._LUT_INFER[(a, b, m)] = None
        
        # FASE 2: LUTs que dependen de (A, B, R) - 27 iteraciones
        for a in states:
            for b in states:
                for r in states:
                    # LUT para learn(): (A, B, R) -> M
                    if a is None or b is None or r is None:
                        TrigateConLut._LUT_LEARN[(a, b, r)] = None
                    elif (a ^ b) == r:
                        TrigateConLut._LUT_LEARN[(a, b, r)] = 1
                    elif (1 - (a ^ b)) == r:
                        TrigateConLut._LUT_LEARN[(a, b, r)] = 0
                    else:
                        TrigateConLut._LUT_LEARN[(a, b, r)] = None
                    
                    # LUT para synthesize(): (A, B, R) -> S
                    if a is None or b is None or r is None:
                        TrigateConLut._LUT_SYNTH[(a, b, r)] = None
                    elif r == 1:
                        temp1 = r ^ a
                        temp2 = r ^ b
                        TrigateConLut._LUT_SYNTH[(a, b, r)] = temp1 ^ temp2
                    elif r == 0:
                        temp1 = 1 - (r ^ a)  # XNOR
                        temp2 = 1 - (r ^ b)  # XNOR
                        TrigateConLut._LUT_SYNTH[(a, b, r)] = temp1 ^ temp2
                    else:
                        TrigateConLut._LUT_SYNTH[(a, b, r)] = None
        
        # FASE 3: LUTs que dependen de (A, M, R) y (B, M, R) - 54 iteraciones total
        for a in states:
            for m in states:
                for r in states:
                    # LUT para deduce_B(): (A, M, R) -> B
                    if a is None or m is None or r is None:
                        TrigateConLut._LUT_DEDUCE_B[(a, m, r)] = None
                    elif m == 1:
                        TrigateConLut._LUT_DEDUCE_B[(a, m, r)] = a ^ r
                    elif m == 0:
                        TrigateConLut._LUT_DEDUCE_B[(a, m, r)] = 1 - (a ^ r)
                    else:
                        TrigateConLut._LUT_DEDUCE_B[(a, m, r)] = None
        
        for b in states:
            for m in states:
                for r in states:
                    # LUT para deduce_A(): (B, M, R) -> A
                    if b is None or m is None or r is None:
                        TrigateConLut._LUT_DEDUCE_A[(b, m, r)] = None
                    elif m == 1:
                        TrigateConLut._LUT_DEDUCE_A[(b, m, r)] = b ^ r
                    elif m == 0:
                        TrigateConLut._LUT_DEDUCE_A[(b, m, r)] = 1 - (b ^ r)
                    else:
                        TrigateConLut._LUT_DEDUCE_A[(b, m, r)] = None
        
        # üîß FIX: Convertir a read-only y validar tama√±o
        expected_size = len(states) ** 3  # 27
        assert len(TrigateConLut._LUT_INFER) == expected_size, f"LUT_INFER size mismatch: {len(TrigateConLut._LUT_INFER)} != {expected_size}"
        
        # Total: 27*4_m√©todos + 27*2_deduce = 135 iteraciones (correcta optimizaci√≥n vs 405)

    def infer(self, A, B, M):
        """Inferencia optimizada con LUT ternaria"""
        return [TrigateConLut._LUT_INFER.get((a, b, m), None) 
                for a, b, m in zip(A, B, M)]
    
    def learn(self, A, B, R):
        """Aprendizaje optimizado con LUT ternaria"""
        return [TrigateConLut._LUT_LEARN.get((a, b, r), None) 
                for a, b, r in zip(A, B, R)]
    
    def synthesize(self, A, B, R=None):
        """S√≠ntesis optimizada con LUT ternaria"""
        if R is None:
            # Modo de s√≠ntesis AB: retorna (M, S)
            M = [TernaryLogic.ternary_xor(a, b) for a, b in zip(A, B)]
            S = [TernaryLogic.ternary_xnor(a, b) for a, b in zip(A, B)]
            return M, S
        else:
            # Modo de s√≠ntesis ABR: retorna S
            return [TrigateConLut._LUT_SYNTH.get((a, b, r), None) 
                    for a, b, r in zip(A, B, R)]
    
    def deduce_B(self, A, M, R):
        """Deducci√≥n de B optimizada con LUT ternaria"""
        return [TrigateConLut._LUT_DEDUCE_B.get((a, m, r), None) 
                for a, m, r in zip(A, M, R)]
    
    def deduce_A(self, B, M, R):
        """Deducci√≥n de A optimizada con LUT ternaria"""
        return [TrigateConLut._LUT_DEDUCE_A.get((b, m, r), None) 
                for b, m, r in zip(B, M, R)]

class Trigate:
    """
    Trigate can√≥nico Aurora con l√≥gica ternaria completa.
    Implementa los tres modos operativos fundamentales con Honestidad Computacional.
    """
    
    def __init__(self, use_lut=True):
        """
        Args:
            use_lut: Si True, usa optimizaci√≥n LUT; si False, usa c√°lculo directo
        """
        self.use_lut = use_lut
        if use_lut:
            self.lut_engine = TrigateConLut()
        
        # Propiedades de estado
        self.last_operation = None
        self.last_inputs = None
        self.last_result = None
        
        self.A = None
        self.B = None
        self.M = None
        self.R = None
        self.S = None
    
    def infer(self, A, B, M):
        """Modo 1: Inferencia - Calcula R desde A, B, M"""
        if self.use_lut:
            return self.lut_engine.infer(A, B, M)
        
        # Implementaci√≥n directa con Honestidad Computacional
        self.A, self.B, self.M = list(A), list(B), list(M)
        
        R = []
        for i in range(3):
            if A[i] is None or B[i] is None or M[i] is None:
                r_bit = None  # Propagaci√≥n de incertidumbre
            elif M[i] == 1:
                r_bit = A[i] ^ B[i]
            elif M[i] == 0:
                r_bit = 1 - (A[i] ^ B[i])
            else:
                r_bit = None
            R.append(r_bit)
        
        self.R = R
        self.last_operation = "infer"
        return R
    
    def learn(self, A, B, R):
        """Modo 2: Aprendizaje - Descubre M desde A, B, R"""
        if self.use_lut:
            return self.lut_engine.learn(A, B, R)
        
        self.A, self.B, self.R = list(A), list(B), list(R)
        
        M = []
        for i in range(3):
            if A[i] is None or B[i] is None or R[i] is None:
                M.append(None)
            elif (A[i] ^ B[i]) == R[i]:
                M.append(1)  # XOR fue usado
            elif (1 - (A[i] ^ B[i])) == R[i]:
                M.append(0)  # XNOR fue usado
            else:
                M.append(None)  # Incoherencia detectada
        
        self.M = M
        self.last_operation = "learn"
        return M
    
    def synthesize(self, A, B, R=None):
        """S√≠ntesis Aurora: combina vectores seg√∫n arquitectura can√≥nica"""
        if self.use_lut:
            return self.lut_engine.synthesize(A, B, R)
        
        if R is None:
            # S√≠ntesis AB: genera (M, S)
            M = [TernaryLogic.ternary_xor(a, b) for a, b in zip(A, B)]
            S = [TernaryLogic.ternary_xnor(a, b) for a, b in zip(A, B)]
            return M, S
        else:
            # S√≠ntesis ABR: genera S
            S = []
            for i in range(3):
                if A[i] is None or B[i] is None or R[i] is None:
                    s_bit = None
                elif R[i] == 1:
                    temp1 = R[i] ^ A[i]
                    temp2 = R[i] ^ B[i]
                    s_bit = temp1 ^ temp2
                elif R[i] == 0:
                    temp1 = 1 - (R[i] ^ A[i])  # XNOR
                    temp2 = 1 - (R[i] ^ B[i])  # XNOR
                    s_bit = temp1 ^ temp2
                else:
                    s_bit = None
                S.append(s_bit)
            return S

class Transcender:
    def __init__(self):
        self.trigate_AB = Trigate()
        self.trigate_BC = Trigate()
        self.trigate_CA = Trigate()
        self.trigate_superior = Trigate()
        self.trigate_Ms = Trigate()

    def infer_R(self, A, B, M_observed):
        """
        Infiere el vector R que conecta A y B para producir M_observed.
        Esta es la versi√≥n corregida del m√©todo 'deep_learning'.
        """
        if len(A) != 3 or len(B) != 3 or len(M_observed) != 3:
            return {'error': 'Todos los vectores deben tener longitud 3.'}
            
        R_hipotesis = []
        for i in range(3):
            if A[i] is None or B[i] is None or M_observed[i] is None:
                R_hipotesis.append(None)
            else:
                # Usando la f√≥rmula R = A ^ B ^ M_observed
                r_bit = A[i] ^ B[i] ^ M_observed[i]
                R_hipotesis.append(r_bit)
            
        return {'R_hipotesis': R_hipotesis}
    
    def compute(self, A, B, C):
        self.A = list(A)
        self.B = list(B)
        self.C = list(C)

        self.M_AB, self.S_AB = self.trigate_AB.synthesize(self.A, self.B)
        self.M_BC, self.S_BC = self.trigate_BC.synthesize(self.B, self.C)
        self.M_CA, self.S_CA = self.trigate_CA.synthesize(self.C, self.A)

        self.M_emergent, self.S_emergent = self.trigate_superior.synthesize(self.M_AB, self.M_BC)

        interm1, _ = self.trigate_Ms.synthesize(self.M_AB, self.M_BC)
        self.M_intermediate, _ = self.trigate_Ms.synthesize(interm1, self.M_CA)

        # üîß FIX CR√çTICO: Protecci√≥n robusta contra None en MetaM con logging
        self.MetaM = []
        none_count = 0
        for i, (a, b) in enumerate(zip(self.M_intermediate, self.M_emergent)):
            if a is None or b is None:
                self.MetaM.append(None)  # Honestidad Computacional
                none_count += 1
            else:
                # Cast expl√≠cito para evitar errores de tipo
                try:
                    self.MetaM.append(int(a) ^ int(b))
                except (ValueError, TypeError):
                    self.MetaM.append(None)
                    none_count += 1
        
        # üö® Warning estructurado si hay cascada de None
        if none_count > 0:
            import warnings
            warnings.warn(f"MetaM cascade: {none_count}/3 bits are None. Consider re-injection via Extender.", 
                         UserWarning, stacklevel=2)

        return {
            'A': self.A,
            'B': self.B,
            'C': self.C,
            'M_AB': self.M_AB,
            'S_AB': self.S_AB,
            'M_BC': self.M_BC,
            'S_BC': self.S_BC,
            'M_CA': self.M_CA,
            'S_CA': self.S_CA,
            'M_emergent': self.M_emergent,
            'S_emergent': self.S_emergent,
            'M_intermediate': self.M_intermediate,
            'MetaM': self.MetaM,
            'none_cascade_warning': none_count > 0
        }

    def compute_fractal(self, fractal_tensor):
        """
        üîß ENHANCED: Procesa un FractalTensor con validaci√≥n robusta de niveles.
        """
        resultados = {}
        
        # üîß FIX: Asegurar que nivel_27 tiene exactamente 27 vectores
        nivel_27 = fractal_tensor.nivel_27 or [[None, None, None]] * 27
        while len(nivel_27) < 27:
            nivel_27.append([None, None, None])
        nivel_27 = nivel_27[:27]  # Truncar si tiene m√°s de 27
        
        # 1. Procesar nivel_27 ‚Üí nivel_9
        nivel_9 = []
        nivel_27_results = []
        for i in range(0, 27, 3):
            trio = nivel_27[i:i+3]
            if len(trio) < 3:
                while len(trio) < 3:
                    trio.append([None, None, None])
            res = self.compute(*trio)
            nivel_9.append(res['M_emergent'])
            nivel_27_results.append(res)
        resultados['nivel_27'] = nivel_27_results
        
        # üîß FIX: Asegurar que nivel_9 tiene exactamente 9 vectores
        while len(nivel_9) < 9:
            nivel_9.append([None, None, None])
        nivel_9 = nivel_9[:9]
        
        # 2. Procesar nivel_9 ‚Üí nivel_3
        nivel_9_results = []
        nivel_3 = []
        for i in range(0, 9, 3):
            trio = nivel_9[i:i+3]
            if len(trio) < 3:
                while len(trio) < 3:
                    trio.append([None, None, None])
            res = self.compute(*trio)
            nivel_3.append(res['M_emergent'])
            nivel_9_results.append(res)
        resultados['nivel_9'] = nivel_9_results
        
        # 3. Procesar nivel_3 ‚Üí vector final
        if len(nivel_3) == 3:
            res_final = self.compute(*nivel_3)
            resultados['nivel_3'] = [res_final]
        else:
            while len(nivel_3) < 3:
                nivel_3.append([None, None, None])
            res_final = self.compute(*nivel_3)
            resultados['nivel_3'] = [res_final]
        resultados['final'] = res_final
        return resultados
class FractalTensor:
    """
    Representa un tensor fractal con 3 niveles:
    - nivel_3: lista de 3 vectores ternarios (cada uno de 3 elementos)
    - nivel_9: lista de 9 vectores ternarios (cada uno de 3 elementos)
    - nivel_27: lista de 27 vectores ternarios (cada uno de 3 elementos)
    """
    def __init__(self, nivel_3=None, nivel_9=None, nivel_27=None):
        self.nivel_3 = nivel_3 if nivel_3 is not None else [[None, None, None] for _ in range(3)]
        self.nivel_9 = nivel_9 if nivel_9 is not None else [[None, None, None] for _ in range(9)]
        self.nivel_27 = nivel_27 if nivel_27 is not None else [[None, None, None] for _ in range(27)]

    @staticmethod
    def random():
        import random
        def rand_vec():
            return [random.choice([0, 1, None]) for _ in range(3)]
        return FractalTensor(
            nivel_3=[rand_vec() for _ in range(3)],
            nivel_9=[rand_vec() for _ in range(9)],
            nivel_27=[rand_vec() for _ in range(27)]
        )

    def as_dict(self):
        return {'nivel_3': self.nivel_3, 'nivel_9': self.nivel_9, 'nivel_27': self.nivel_27}

    def __repr__(self):
        return f"FractalTensor(3={self.nivel_3}, 9={self.nivel_9}, 27={self.nivel_27})"


class Evolver:
    """
    Evolver unificado Aurora - combina Arquetipo, Din√°mica y Relator.
    Analiza patrones emergentes y formaliza axiomas del sistema.
    """
    
    def __init__(self):
        self.base_transcender = Transcender()
        self.meta_transcender = Transcender()
        self.axioms = []
        self.archetypal_patterns = {}
        
        # üîß NEW: Tracker de rotaci√≥n √°urea
        try:
            from utils_golden import GoldenRotationTracker, TensorPoolManager
            self.golden_tracker = GoldenRotationTracker()
            self.tensor_pool = TensorPoolManager()
        except ImportError:
            self.golden_tracker = None
            self.tensor_pool = None
    
    # ========== ARQUITEIPO ==========
    
    def compute_archetypes(self, *tensors, nivel='nivel_3', use_rotation=True):
        """
        üîß ENHANCED: Descubre arquetipos con rotaci√≥n h√≠brida œÜ/Fibonacci.
        Implementa exploraci√≥n estratificada del espacio tensorial.
        """
        # üîß FIX: Silenciar logs por defecto
        if hasattr(self, 'verbose') and self.verbose:
            print(f"[ARQUETIPO] Descubriendo patrones arquetipos en {nivel}...")
        
        # üîß NEW: Si tenemos pool manager, usar rotaci√≥n estrat√©gica
        if use_rotation and self.tensor_pool and len(tensors) >= 3:
            # A√±adir tensores al pool para gesti√≥n
            for tensor in tensors:
                self.tensor_pool.add_tensor(tensor)
            
            # Obtener tr√≠o optimizado para arquetipos
            optimal_trio = self.tensor_pool.get_tensor_trio(task_type="arquetipo")
            if len(optimal_trio) >= 3:
                tensors = optimal_trio[:3]
                print(f"[ARQUETIPO] Usando tr√≠o rotado estrat√©gicamente")
        
        metaMs = []
        for ft in tensors:
            results = self.base_transcender.compute_fractal(ft)
            metaMs_nivel = [res['MetaM'] for res in results[nivel] if res and 'MetaM' in res]
            if metaMs_nivel:
                metaMs.append(metaMs_nivel[-1])  # √öltimo MetaM sintetizado
        
        if len(metaMs) < 3:
            return {'error': 'Se requieren al menos 3 tensores para arquetipo global'}
        
        # Validar coherencia antes de computar arquetipo
        if any(m is None or any(x is None for x in m) for m in metaMs):
            return {'error': 'MetaMs incompletos - violaci√≥n de Honestidad Computacional'}
        
        # ‚îÄ‚îÄ üîß ROTACI√ìN √ÅUREA CL√ÅSICA (mantenida para compatibilidad) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        rotation_applied = False
        metaMs_phi = metaMs[:3]
        
        try:
            from utils_golden import golden_rotate
            # Rotar metaMs usando n√∫mero de arquetipos existentes como seed
            metaMs_phi = golden_rotate(metaMs[:3], steps=len(self.archetypal_patterns))
            rotation_applied = True
        except ImportError:
            pass
        
        # Computar arquetipo usando meta-transcender
        arquetipo = self.meta_transcender.compute(*metaMs_phi)
        
        # Almacenar patr√≥n arquet√≠pico con clave rotada
        arquetipo_key = tuple(map(tuple, metaMs_phi))
        self.archetypal_patterns[arquetipo_key] = arquetipo
        
        # üîß NEW: Tracking de rotaci√≥n con pool stats
        if self.golden_tracker and rotation_applied:
            quality_score = 1.0 if arquetipo.get('MetaM') else 0.0
            self.golden_tracker.record_rotation(
                len(self.archetypal_patterns), 
                'arquetipo_hybrid', 
                quality_score
            )
        
        # üîß FIX: Solo imprimir si est√° en modo verbose
        if hasattr(self, 'verbose') and self.verbose:
            print(f"[ARQUETIPO] Patr√≥n descubierto: {arquetipo['MetaM']} "
                  f"{'(œÜ-h√≠brido)' if rotation_applied else ''}")
        return arquetipo
    
    # ========== DIN√ÅMICA ==========
    
    def analyze_dynamics_adaptive(self, sequence, metric='auto', window=3, use_rotation=True):
        """
        üîß ENHANCED: An√°lisis din√°mico con rotaci√≥n Fibonacci para multi-escala.
        """
        if not sequence or len(sequence) < 2:
            return {'error': 'Secuencia insuficiente para an√°lisis din√°mico'}
        
        # üîß NEW: Si tenemos pool, usar rotaci√≥n Fibonacci para din√°micas
        enhanced_sequence = sequence
        rotation_method = "none"
        
        if use_rotation and self.tensor_pool:
            try:
                from utils_golden import TensorRotor
                # üîß FIX: Usar hybrid en lugar de fibonacci para secuencias peque√±as
                mode = "hybrid" if len(sequence) <= 5 else "fibonacci"
                dynamic_rotor = TensorRotor(len(sequence), mode=mode)
                
                # Obtener secuencia rotada para an√°lisis multi-escala
                rotated_indices = []
                for _ in range(min(len(sequence), 5)):  # M√°ximo 5 elementos
                    rotated_indices.append(dynamic_rotor.next())
                
                enhanced_sequence = [sequence[i] for i in rotated_indices if i < len(sequence)]
                rotation_method = mode
                
                # üîß FIX: Solo imprimir si est√° en modo verbose
                if hasattr(self, 'verbose') and self.verbose:
                    print(f"[DIN√ÅMICA] Usando rotaci√≥n {mode}: {rotated_indices}")
                
            except ImportError:
                pass
        
        # Selecci√≥n adaptativa de m√©trica
        if metric == 'auto':
            hamming_vars = self._compute_variance(enhanced_sequence, 'hamming')
            euclidean_vars = self._compute_variance(enhanced_sequence, 'euclidean')
            metric = 'hamming' if hamming_vars >= euclidean_vars else 'euclidean'
        
        # Calcular diferencias temporales con manejo seguro de None
        diffs = []
        for i in range(1, len(enhanced_sequence)):
            a, b = enhanced_sequence[i-1], enhanced_sequence[i]
            if metric == 'hamming':
                diff = sum(
                    1 for x, y in zip(a, b) 
                    if x is not None and y is not None and x != y
                )
            else:  # euclidean
                valid_pairs = [
                    (float(x), float(y)) for x, y in zip(a, b) 
                    if x is not None and y is not None
                ]
                if valid_pairs:
                    diff = sum((x - y) ** 2 for x, y in valid_pairs) ** 0.5
                else:
                    diff = 0.0
            diffs.append(diff)
        
        # Umbral robusto usando desviaci√≥n est√°ndar
        if len(diffs) > 1:
            mean_diff = sum(diffs) / len(diffs)
            variance = sum((d - mean_diff) ** 2 for d in diffs) / len(diffs)
            stdev = variance ** 0.5
            threshold = mean_diff + 2 * stdev
        else:
            mean_diff = diffs[0] if diffs else 0
            threshold = mean_diff * 1.5
        
        rupturas = [i for i, d in enumerate(diffs) if d > threshold]
        ciclos = self._detect_cycles(enhanced_sequence, window)
        
        # Clasificaci√≥n arquet√≠pica mejorada
        rupture_ratio = len(rupturas) / len(diffs) if diffs else 0
        if rupture_ratio > 0.5:
            arquetipo_dinamico = 'ca√≥tico'
        elif len(ciclos) > 0:
            arquetipo_dinamico = 'c√≠clico'
        elif mean_diff < 0.1:
            arquetipo_dinamico = 'est√°tico'
        else:
            arquetipo_dinamico = 'din√°mico'
        
        return {
            'metric_used': metric,
            'rotation_method': rotation_method,
            'enhanced_sequence_length': len(enhanced_sequence),
            'temporal_diffs': diffs,
            'mean_change': mean_diff,
            'stdev': stdev if len(diffs) > 1 else 0,
            'threshold_used': threshold,
            'rupturas_detectadas': rupturas,
            'ciclos_detectados': ciclos,
            'arquetipo_dinamico': arquetipo_dinamico,
            'coherencia_temporal': 1.0 - rupture_ratio,
            'multi_scale_analysis': rotation_method == "fibonacci"
        }
    
    # ========== RELATOR ==========
    
    def relate_vectors(self, vectors, context_id=None, distance_metric='auto', 
                      affinity_threshold=None, use_rotation=True):
        """
        üîß ENHANCED: An√°lisis relacional con rotaci√≥n h√≠brida para clustering.
        """
        if not vectors or len(vectors) < 2:
            return {'error': 'Se requieren al menos dos vectores para an√°lisis relacional'}
        
        # üîß NEW: Rotaci√≥n para clustering optimizado
        enhanced_vectors = vectors
        rotation_method = "none"
        
        if use_rotation and len(vectors) >= 3:
            try:
                from utils_golden import TensorRotor
                # Usar modo h√≠brido para relatores (balance cobertura/diversidad)
                relator_rotor = TensorRotor(len(vectors), mode="hybrid")
                
                # Obtener √≠ndices para an√°lisis relacional optimizado
                rotor_indices = []
                for _ in range(min(len(vectors), 7)):  # M√°ximo 7 para relatores
                    rotor_indices.append(relator_rotor.next())
                
                enhanced_vectors = [vectors[i] for i in rotor_indices if i < len(vectors)]
                rotation_method = "hybrid"
                
                # üîß FIX: Solo imprimir si est√° en modo verbose
                if hasattr(self, 'verbose') and self.verbose:
                    print(f"[RELATOR] Usando rotaci√≥n h√≠brida: {rotor_indices}")
                
            except ImportError:
                pass
        
        # Selecci√≥n adaptativa de m√©trica
        if distance_metric == 'auto':
            hamming_spread = self._compute_relational_spread(enhanced_vectors, 'hamming')
            euclidean_spread = self._compute_relational_spread(enhanced_vectors, 'euclidean')
            distance_metric = 'hamming' if hamming_spread >= euclidean_spread else 'euclidean'
        
        # Matriz de distancias relacionales
        n = len(enhanced_vectors)
        dist_matrix = [[0] * n for _ in range(n)]
        for i in range(n):
            for j in range(i + 1, n):
                if distance_metric == 'hamming':
                    dist = sum(a != b for a, b in zip(enhanced_vectors[i], enhanced_vectors[j]))
                else:
                    dist = sum(((a - b) ** 2) if (a is not None and b is not None) else 0 
                              for a, b in zip(enhanced_vectors[i], enhanced_vectors[j])) ** 0.5
                dist_matrix[i][j] = dist_matrix[j][i] = dist
        
        # Umbral adaptativo de afinidad
        all_distances = [dist_matrix[i][j] for i in range(n) for j in range(i + 1, n)]
        if affinity_threshold is None:
            affinity_threshold = sum(all_distances) / len(all_distances) if all_distances else 1
        
        # Clustering por afinidad
        clusters = self._cluster_by_affinity(enhanced_vectors, dist_matrix, affinity_threshold)
        outliers = [c[0] for c in clusters if len(c) == 1]
        
        # Emergencia de vector superior
        emergent_vector = None
        emergence_condition = all(
            isinstance(v, (list, tuple)) and len(v) == 3 and 
            all(x is not None for x in v) for v in enhanced_vectors
        )
        
        if emergence_condition:
            emergent_vector = []
            for i in range(3):
                vals = [v[i] for v in enhanced_vectors]
                emergent_bit = vals[0]
                for b in vals[1:]:
                    emergent_bit ^= b
                emergent_vector.append(emergent_bit)
        
        return {
            'context_id': context_id,
            'distance_metric': distance_metric,
            'rotation_method': rotation_method,
            'enhanced_vectors_count': len(enhanced_vectors),
            'dist_matrix': dist_matrix,
            'clusters': clusters,
            'outliers': outliers,
            'affinity_threshold': affinity_threshold,
            'emergent_vector': emergent_vector,
            'emergence_condition': emergence_condition,
            'relational_coherence': 1.0 - (len(outliers) / n) if n > 0 else 0.0,
            'cluster_optimization': rotation_method in ["hybrid", "phi"]
        }
    
    # ========== M√âTRICAS DE ROTACI√ìN ==========
    
    def get_rotation_stats(self) -> dict:
        """
        üîß NEW: Estad√≠sticas completas de rotaci√≥n y pools.
        
        Returns:
            dict: M√©tricas de eficiencia de rotaci√≥n
        """
        stats = {
            'golden_tracker': None,
            'tensor_pools': None,
            'rotation_enabled': False
        }
        
        if self.golden_tracker:
            stats['golden_tracker'] = self.golden_tracker.get_stats()
            stats['rotation_enabled'] = True
        
        if self.tensor_pool:
            stats['tensor_pools'] = self.tensor_pool.get_pool_stats()
        
        return stats
    
    def optimize_rotation_strategy(self):
        """
        üîß NEW: Optimiza estrategias de rotaci√≥n bas√°ndose en m√©tricas.
        """
        if not self.tensor_pool:
            return
        
        # Optimizar rotors bas√°ndose en patrones de acceso
        self.tensor_pool.optimize_rotors()
        
        print("[EVOLVER] Estrategias de rotaci√≥n optimizadas")

class Extender:
    """
    Extender Aurora mejorado - Ingeniero y Arquitecto de reconstrucci√≥n.
    Usa axiomas, din√°micas y relaciones para reconstrucci√≥n l√≥gica guiada.
    """
    
    def __init__(self, knowledge_base=None, evolver=None):
        self.knowledge_base = knowledge_base or KnowledgeBase()
        self.evolver = evolver or Evolver()
        self.transcender = Transcender()
        # üîß NEW: Contador para rotaciones √°ureas
        self.rotation_counter = 0

    def rebuild(self, ms, metam, ss):
        """
        üîß ENHANCED: Reconstrucci√≥n con soporte aritm√©tico, geom√©trico y cuadr√°tico.
        
        Arguments:
        ---------
        ms, metam : Utilizados para an√°lisis de axiomas y din√°micas futuras
        ss        : [Œî‚ÇÅ, ŒîŒî, None] - falta el tercer delta
        
        Returns:
        --------
        Lista [Œî‚ÇÅ, ŒîŒî, Œî‚ÇÇ] con el tercer delta reconstruido, o None si falla
        """
        try:
            # Validaci√≥n de entrada con Honestidad Computacional
            if not ss or len(ss) < 3:
                return None
            
            # üîß FIX: Copia inmutable para evitar efectos laterales
            ss_copy = list(ss)
            delta_1, delta_delta, missing = ss_copy
            
            # Si ya est√° completo, validar coherencia
            if missing is not None:
                return ss_copy
                
            # Propagaci√≥n de NULL si falta informaci√≥n cr√≠tica
            if delta_1 is None or delta_delta is None:
                return None
            
            # FASE 1: An√°lisis de axiomas (usando Evolver)
            axiom = "Axioma por defecto: progresi√≥n din√°mica"
            if ms and metam:
                try:
                    axiom_analysis = self.evolver.analyze_metaMs([ms, metam, [delta_1, delta_delta, 0]])
                    axiom = self.evolver.formalize_axiom(axiom_analysis, [ms, metam])
                except:
                    pass  # Usar axioma por defecto
            
            # FASE 2: üîß ENHANCED - Detecci√≥n mejorada de tipo de progresi√≥n
            progression_type = "arithmetic"  # default
            
            # üîß FIX: Detectar progresi√≥n geom√©trica por flag
            if delta_delta == 1:  # Flag que pusimos en Ss_pattern durante ingest
                ratio = delta_1  # delta_1 ya es la raz√≥n
                progression_type = "geometric"
                delta_2 = ratio  # Para geom√©tricas, el siguiente "delta" es la raz√≥n misma
            elif delta_1 != 0 and delta_delta != 0 and delta_delta % delta_1 == 0:
                # Fallback: detectar geom√©trica por patr√≥n matem√°tico
                ratio = (delta_1 + delta_delta) // delta_1
                if ratio > 1:  # progresi√≥n geom√©trica real
                    progression_type = "geometric"
                    delta_2 = delta_1 * ratio
                else:
                    # ratio == 1 degenera a aritm√©tica
                    progression_type = "arithmetic"
                    delta_2 = delta_1 + delta_delta
            elif delta_delta == 0:
                # progresi√≥n aritm√©tica pura (constante)
                progression_type = "arithmetic_constant"
                delta_2 = delta_1
            elif abs(delta_delta) > abs(delta_1) * 2:
                # aceleraci√≥n cuadr√°tica/din√°mica
                progression_type = "quadratic"
                delta_2 = delta_1 + delta_delta
            else:
                # progresi√≥n aritm√©tica est√°ndar
                progression_type = "arithmetic"
                delta_2 = delta_1 + delta_delta
            
            # FASE 3: An√°lisis din√°mico para validaci√≥n
            sequence = [delta_1, delta_delta]
            if len(sequence) >= 2:
                try:
                    dynamic_analysis = self.evolver.analyze_dynamics_adaptive(
                        [[sequence[0]], [sequence[1]]], window=2
                    )
                    dynamic_type = dynamic_analysis.get('arquetipo_dinamico', 'lineal')
                except:
                    dynamic_type = 'lineal'
            else:
                dynamic_type = 'lineal'
            
            # FASE 4: Ajuste final basado en an√°lisis din√°mico
            if dynamic_type == 'c√≠clico' and progression_type != "geometric":
                # Patr√≥n c√≠clico detectado - ajustar
                delta_2 = delta_1  # Retorno al valor inicial
                progression_type = "cyclic"
            elif dynamic_type == 'ca√≥tico':
                # Uso de proporci√≥n √°urea para estabilizar
                phi = (1 + 5 ** 0.5) / 2
                delta_2 = int(delta_1 + delta_delta / phi)
                progression_type = "chaotic_stabilized"
            
            # FASE 5: Validaci√≥n de coherencia final
            reconstructed = [delta_1, delta_delta, delta_2]
            
            # üîß FIX: Verificaci√≥n robusta de resultado
            if any(x is None for x in reconstructed[:2]):
                return None
            
            # Validaci√≥n de rango razonable (evitar explosi√≥n num√©rica)
            if abs(delta_2) > 10000:  # Threshold de seguridad
                reconstructed = [delta_1, delta_delta, delta_1 + delta_delta]  # Fallback lineal
                progression_type = "fallback_linear"
            
            # Agregar metadatos de diagn√≥stico (opcional)
            if hasattr(self, '_debug_mode') and self._debug_mode:
                reconstructed._progression_type = progression_type
                reconstructed._axiom_used = axiom
                reconstructed._dynamic_type = dynamic_type
            
            return reconstructed
            
        except Exception as e:
            # Fallback con Honestidad Computacional + logging
            import warnings
            warnings.warn(f"Extender.rebuild failed: {e}. Returning None.", UserWarning)
            return None

    def extend(self, Ss, contexto=None):
        """
        üîß ENHANCED: Extensi√≥n arquitectural con rotaci√≥n √°urea y L Spaces.
        Implementa el flujo completo: Œ¶-Rotation -> Arquetipo -> Din√°mica -> Relator -> Reconstrucci√≥n.
        """
        # Validaci√≥n de entrada
        if not isinstance(Ss, list) or len(Ss) < 3:
            return {'error': 'Se requieren al menos 3 elementos en Ss para extensi√≥n arquitectural'}
        
        for idx, v in enumerate(Ss[:3]):
            if not isinstance(v, list) or len(v) != 3:
                return {'error': f'Elemento Ss[{idx}] debe ser vector de 3 elementos'}
        
        # üîß NEW: Determinar space_id del contexto
        space_id = "default"
        if contexto:
            space_id = contexto.get("space_id", "default")
            # Mapeo inteligente de contexto a espacio l√≥gico
            if "tipo_secuencia" in contexto:
                tipo = contexto["tipo_secuencia"]
                if tipo == "geom":
                    space_id = "geometric"
                elif tipo == "cycle":
                    space_id = "cyclic"
                elif tipo == "arith":
                    space_id = "arithmetic"
        
        # ‚îÄ‚îÄ üîß ROTACI√ìN √ÅUREA PREVIA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        rotation_applied = False
        Ss_rot = Ss
        
        try:
            from utils_golden import golden_rotate, PHI
            # Incrementar contador y aplicar rotaci√≥n √°urea
            self.rotation_counter += 1
            phi_steps = int(len(Ss) * PHI * self.rotation_counter) % len(Ss)
            Ss_rot = golden_rotate(Ss, steps=phi_steps)
            rotation_applied = True
        except ImportError:
            pass  # Continuar sin rotaci√≥n si utils_golden no est√° disponible
        
        # FASE 1: An√°lisis arquet√≠pico (con rotaci√≥n √°urea aplicada)
        try:
            # Convertir Ss rotados a tensores para an√°lisis arquet√≠pico
            ft_temp = FractalTensor(nivel_3=Ss_rot[:3])
            arquetipo_analysis = self.evolver.compute_archetypes(ft_temp)
            arquetipo_usado = arquetipo_analysis.get('MetaM', 'arquetipo_phi' if rotation_applied else 'arquetipo_indeterminado')
            
            # üîß NEW: Guardar arquetipo en space "meta" si es v√°lido
            if arquetipo_analysis and 'MetaM' in arquetipo_analysis:
                try:
                    axiom_id = f"arch_{hash(str(arquetipo_analysis['MetaM'])):x}"
                    if rotation_applied:
                        axiom_id += f"_phi{self.rotation_counter}"
                    
                    self.knowledge_base.add_entry(
                        space_id="meta",
                        A=[0,0,0], B=[0,0,0], C=[0,0,0],
                        M_emergent=arquetipo_analysis['M_emergent'],
                        MetaM=arquetipo_analysis['MetaM'],
                        R_validos=[[0,0,None]],
                        transcender_id=axiom_id
                    )
                except:
                    pass  # No cr√≠tico si falla el guardado
        except:
            arquetipo_usado = 'arquetipo_fallback'
        
        # FASE 2: An√°lisis din√°mico de la secuencia Ss (usar originales para coherencia)
        dynamic_analysis = self.evolver.analyze_dynamics_adaptive(Ss[:3])
        dynamic_pattern = dynamic_analysis.get('arquetipo_dinamico', 'indeterminado')
        
        # FASE 3: An√°lisis relacional (usar rotados para diversidad)
        relational_analysis = self.evolver.relate_vectors(Ss_rot[:3])
        emergent_vector = relational_analysis.get('emergent_vector')
        coherencia_relacional = relational_analysis.get('relational_coherence', 0)
        
        # FASE 4: üîß ENHANCED - B√∫squeda en Knowledge Base con L Spaces
        detalles_kb = []
        detalles_meta = []
        
        if self.knowledge_base and hasattr(self.knowledge_base, 'all_entries'):
            try:
                # Buscar en el espacio espec√≠fico
                space_entries = self.knowledge_base.all_entries(space_id=space_id)
                for entry in space_entries[:10]:
                    if isinstance(entry, dict) and 'MetaM' in entry:
                        detalles_kb.append(entry)
                
                # Buscar en el espacio meta para arquetipos
                meta_entries = self.knowledge_base.all_entries(space_id="meta")
                for entry in meta_entries[:5]:
                    if isinstance(entry, dict) and 'MetaM' in entry:
                        detalles_meta.append(entry)
            except:
                # Fallback a b√∫squeda sin space_id
                try:
                    all_entries = self.knowledge_base.all_entries()
                    for entry in all_entries[:10]:
                        if isinstance(entry, dict) and 'MetaM' in entry:
                            detalles_kb.append(entry)
                except:
                    pass
        
        # FASE 5: Reconstrucci√≥n arquitectural mejorada
        if emergent_vector and coherencia_relacional > 0.7:
            # Alta coherencia: usar vector emergente
            tensores_reconstruidos = emergent_vector
            metodo_reconstruccion = 'emergencia_relacional'
        elif detalles_meta:
            # Usar arquetipo de espacio meta
            candidato = detalles_meta[0]
            tensores_reconstruidos = candidato.get('M_emergent', Ss[0])
            metodo_reconstruccion = 'meta_arquetipo'
        elif detalles_kb:
            # Usar conocimiento del espacio espec√≠fico
            candidato = detalles_kb[0]
            tensores_reconstruidos = candidato.get('M_emergent', Ss[0])
            metodo_reconstruccion = f'knowledge_base_{space_id}'
        elif dynamic_pattern in ['c√≠clico', 'est√°tico']:
            # Patr√≥n estable: usar primer elemento como base
            tensores_reconstruidos = Ss[0]
            metodo_reconstruccion = 'estabilidad_dinamica'
        else:
            # Reconstrucci√≥n por transcendencia
            try:
                trans_result = self.transcender.compute(*Ss[:3])
                tensores_reconstruidos = trans_result.get('M_emergent', Ss[0])
                metodo_reconstruccion = 'transcendencia'
            except:
                tensores_reconstruidos = Ss[0]  # Fallback seguro
                metodo_reconstruccion = 'fallback'
        
        # FASE 6: Aplicaci√≥n de proporci√≥n √°urea para optimizaci√≥n
        if contexto and 'objetivo' in contexto:
            objetivo = contexto['objetivo']
            phi = (1 + 5 ** 0.5) / 2
            
            # Rotaci√≥n √°urea para optimizar aproximaci√≥n al objetivo
            if isinstance(tensores_reconstruidos, list) and len(tensores_reconstruidos) >= 3:
                mejor_aproximacion = tensores_reconstruidos
                mejor_distancia = sum(abs(a - b) if (a is not None and b is not None) else 0 
                                    for a, b in zip(tensores_reconstruidos, objetivo))
                
                # Probar rotaciones √°ureas
                for rot_factor in [1/phi, phi-1, 1-1/phi]:
                    rot_steps = int(len(tensores_reconstruidos) * rot_factor)
                    rotado = tensores_reconstruidos[rot_steps:] + tensores_reconstruidos[:rot_steps]
                    distancia = sum(abs(a - b) if (a is not None and b is not None) else 0 
                                  for a, b in zip(rotado, objetivo))
                    
                    if distancia < mejor_distancia:
                        mejor_distancia = distancia
                        mejor_aproximacion = rotado
                
                tensores_reconstruidos = mejor_aproximacion
                metodo_reconstruccion += '_optimizado_aureano'
        
        # üîß ENHANCED: A√±adir sufijo œÜ si se aplic√≥ rotaci√≥n
        if rotation_applied:
            metodo_reconstruccion += '_œÜ'
        
        # üîß CRITICAL FIX: Normalizar tensores_reconstruidos para shape (3,3)
        if tensores_reconstruidos and isinstance(tensores_reconstruidos, list):
            # Si es un vector plano [x,y,z], convertir a matriz 3x3
            if len(tensores_reconstruidos) == 3 and not isinstance(tensores_reconstruidos[0], list):
                tensores_reconstruidos = [tensores_reconstruidos] * 3
        
        # Construcci√≥n de respuesta arquitectural
        reconstruccion = {
            'arquetipo_utilizado': arquetipo_usado,
            'patron_dinamico': dynamic_pattern,
            'coherencia_relacional': coherencia_relacional,
            'vector_emergente': emergent_vector,
            'detalles_kb': len(detalles_kb),
            'detalles_meta': len(detalles_meta),
            'space_id': space_id,
            'tensores_reconstruidos': tensores_reconstruidos,
            'metodo_reconstruccion': metodo_reconstruccion,
            'rotation_applied': rotation_applied,
            'rotation_step': self.rotation_counter if rotation_applied else 0,
            'axiomas_aplicados': self.evolver.get_axioms()[-3:] if self.evolver.get_axioms() else []
        }
        
        return {
            'input_Ss': Ss,
            'input_Ss_rotated': Ss_rot if rotation_applied else None,
            'contexto': contexto,
            'reconstruccion': reconstruccion,
            'coherencia_arquitectural': True
        }

class _SingleUniverseKB:
    """
    üîß NEW: Knowledge Base de un solo universo l√≥gico.
    Contiene la implementaci√≥n original de KnowledgeBase.
    """
    
    def __init__(self):
        # Almacenamiento principal: cada entrada es un dict completo
        self.knowledge = []
        
        # √çndices para b√∫squeda eficiente
        self.ms_to_metam = {}  # Ms -> MetaM mapping para validaci√≥n de coherencia
        self.ms_index = {}     # Ms -> lista de entradas para b√∫squeda r√°pida
        
        # Estad√≠sticas de coherencia
        self.coherence_violations = 0
        self.total_entries = 0

    def add_entry(self, A, B, C, M_emergent, MetaM, R_validos, transcender_id=None, Ms=None):
        """Almacenamiento con hash determin√≠stico y validaci√≥n reforzada."""
        # Validaci√≥n de tipos y longitudes
        for name, v in zip(['A','B','C','M_emergent','MetaM'], [A,B,C,M_emergent,MetaM]):
            if not isinstance(v, (list, tuple)) or len(v) != 3:
                raise ValueError(f"{name} debe ser un vector de 3 elementos: {v}")
        
        # Conversi√≥n a listas para consistencia
        A, B, C = list(A), list(B), list(C)
        M_emergent, MetaM = list(M_emergent), list(MetaM)
        
        # Validaci√≥n de Honestidad Computacional
        if any(x is None for x in M_emergent):
            raise ValueError(f"Violaci√≥n de Honestidad Computacional: M_emergent contiene None: {M_emergent}")
        if any(x is None for x in MetaM):
            raise ValueError(f"Violaci√≥n de Honestidad Computacional: MetaM contiene None: {MetaM}")
        
        # Clave de coherencia (Ms como tupla para hashing)
        ms_key = tuple(M_emergent)
        
        # VALIDACI√ìN DE COHERENCIA ABSOLUTA
        if ms_key in self.ms_to_metam:
            stored_metam = self.ms_to_metam[ms_key]
            if stored_metam != tuple(MetaM):
                self.coherence_violations += 1
                raise Exception(
                    f"VIOLACI√ìN DEL PRINCIPIO DE COHERENCIA ABSOLUTA:\n"
                    f"  Ms={M_emergent} ya existe con MetaM={stored_metam}\n"
                    f"  Intento de asociar con MetaM diferente: {MetaM}\n"
                    f"  Cada Ms debe corresponder √öNICAMENTE a un MetaM espec√≠fico.\n"
                    f"  Violaciones detectadas hasta ahora: {self.coherence_violations}"
                )
        else:
            # Registrar nueva correspondencia Ms -> MetaM
            self.ms_to_metam[ms_key] = tuple(MetaM)
        
        # Hash determin√≠stico para trazabilidad y deduplicaci√≥n
        import hashlib
        entry_signature = f"{A}{B}{C}{M_emergent}{MetaM}".encode('utf-8')
        entry_hash = hashlib.sha256(entry_signature).hexdigest()[:16]
        
        # Verificar duplicaci√≥n por hash
        for existing_entry in self.knowledge:
            if existing_entry.get('entry_hash') == entry_hash:
                return  # Exit temprano sin error
        
        # Construcci√≥n de entrada completa con hash
        entry = {
            'A': A, 'B': B, 'C': C,
            'M_emergent': M_emergent,
            'MetaM': MetaM,
            'R_validos': R_validos,
            'transcender_id': transcender_id,
            'timestamp': self._get_timestamp(),
            'entry_hash': entry_hash
        }
        
        # Almacenar en knowledge base
        self.knowledge.append(entry)
        self.total_entries += 1
        
        # Actualizar √≠ndice de b√∫squeda por Ms
        if ms_key not in self.ms_index:
            self.ms_index[ms_key] = []
        self.ms_index[ms_key].append(entry)
    
    def find_by_ms(self, Ms_query, radius=0):
        """B√∫squeda con distancia Hamming que maneja None correctamente."""
        Ms_query = list(Ms_query)
        matches = []
        
        if radius == 0:
            # B√∫squeda exacta optimizada por √≠ndice
            ms_key = tuple(Ms_query)
            if ms_key in self.ms_index:
                matches.extend(self.ms_index[ms_key])
        else:
            # B√∫squeda difusa con manejo correcto de None
            for entry in self.knowledge:
                Ms_stored = entry['M_emergent']
                # Contar solo diferencias v√°lidas
                valid_comparisons = [
                    (a, b) for a, b in zip(Ms_query, Ms_stored) 
                    if a is not None and b is not None
                ]
                
                if not valid_comparisons:
                    continue
                
                distance = sum(1 for a, b in valid_comparisons if a != b)
                if distance <= radius:
                    matches.append(entry)
        
        return matches
    
    def all_entries(self):
        """Retorna todas las entradas almacenadas."""
        return self.knowledge.copy()
    
    def get_coherence_stats(self):
        """Estad√≠sticas mejoradas con detalle de hashes √∫nicos."""
        unique_hashes = set(entry.get('entry_hash', 'no_hash') for entry in self.knowledge)
        
        return {
            'total_entries': self.total_entries,
            'unique_ms_patterns': len(self.ms_to_metam),
            'coherence_violations': self.coherence_violations,
            'coherence_ratio': 1.0 - (self.coherence_violations / max(1, self.total_entries)),
            'ms_to_metam_mappings': len(self.ms_to_metam),
            'unique_hashes': len(unique_hashes),
            'deduplication_savings': self.total_entries - len(unique_hashes)
        }
    
    def validate_global_coherence(self):
        """Validaci√≥n global de coherencia."""
        violations = []
        ms_metam_pairs = {}
        
        for i, entry in enumerate(self.knowledge):
            ms_key = tuple(entry['M_emergent'])
            current_metam = tuple(entry['MetaM'])
            
            if ms_key in ms_metam_pairs:
                stored_metam = ms_metam_pairs[ms_key]
                if stored_metam != current_metam:
                    violations.append({
                        'entry_index': i,
                        'Ms': entry['M_emergent'],
                        'MetaM_stored': list(stored_metam),
                        'MetaM_conflict': entry['MetaM']
                    })
            else:
                ms_metam_pairs[ms_key] = current_metam
        
        return {
            'is_coherent': len(violations) == 0,
            'violations_found': len(violations),
            'violation_details': violations,
            'unique_ms_patterns': len(ms_metam_pairs)
        }
    
    def _get_timestamp(self):
        """Genera timestamp para trazabilidad."""
        import time
        return time.time()
    
    def __len__(self):
        """Retorna n√∫mero total de entradas."""
        return len(self.knowledge)

class KnowledgeBase:
    """
    üîß ENHANCED: Knowledge Base Aurora con L Spaces (Espacios L√≥gicos).
    Implementa multiverso de conocimiento con coherencia por espacio.
    Cada space_id mantiene su propio universo de Ms‚ÜîMetaM sin interferencias.
    """
    
    def __init__(self):
        self.universes = {}  # {space_id: _SingleUniverseKB}
        self.default_space = "default"
    
    def _get_space(self, space_id=None):
        """Obtiene o crea un espacio l√≥gico espec√≠fico."""
        if space_id is None:
            space_id = self.default_space
        
        if space_id not in self.universes:
            self.universes[space_id] = _SingleUniverseKB()
        
        return self.universes[space_id]
    
    # Proxy methods que pasan space_id expl√≠cito
    def add_entry(self, A, B, C, M_emergent, MetaM, R_validos, 
                  transcender_id=None, Ms=None, space_id=None):
        """Proxy para add_entry con soporte de L Spaces."""
        return self._get_space(space_id).add_entry(
            A, B, C, M_emergent, MetaM, R_validos, transcender_id, Ms
        )
    
    def find_by_ms(self, Ms_query, radius=0, space_id=None):
        """Proxy para find_by_ms con soporte de L Spaces."""
        return self._get_space(space_id).find_by_ms(Ms_query, radius)
    
    def all_entries(self, space_id=None):
        """Proxy para all_entries con soporte de L Spaces."""
        return self._get_space(space_id).all_entries()
    
    def get_coherence_stats(self, space_id=None):
        """
        üîß FIX: Proxy para get_coherence_stats con soporte de L Spaces.
        ADDED: Este m√©todo faltaba y causaba AttributeError en benchmark4_fractal.py
        """
        if space_id is None:
            # Estad√≠sticas globales de todos los espacios
            global_stats = {
                'total_entries': 0,
                'unique_ms_patterns': 0,
                'coherence_violations': 0,
                'coherence_ratio': 1.0,
                'spaces_count': len(self.universes),
                'spaces': {}
            }
            
            for sid, universe in self.universes.items():
                space_stats = universe.get_coherence_stats()
                global_stats['total_entries'] += space_stats['total_entries']
                global_stats['unique_ms_patterns'] += space_stats['unique_ms_patterns']
                global_stats['coherence_violations'] += space_stats['coherence_violations']
                global_stats['spaces'][sid] = space_stats
            
            if global_stats['total_entries'] > 0:
                global_stats['coherence_ratio'] = 1.0 - (
                    global_stats['coherence_violations'] / global_stats['total_entries']
                )
            
            return global_stats
        else:
            return self._get_space(space_id).get_coherence_stats()
    
    def validate_global_coherence(self, space_id=None):
        """Proxy para validate_global_coherence con soporte de L Spaces."""
        if space_id is None:
            # Validar coherencia en todos los espacios
            global_report = {
                'is_coherent': True,
                'violations_found': 0,
                'spaces_validated': len(self.universes),
                'spaces_reports': {}
            }
            
            for sid, universe in self.universes.items():
                space_report = universe.validate_global_coherence()
                global_report['spaces_reports'][sid] = space_report
                
                if not space_report['is_coherent']:
                    global_report['is_coherent'] = False
                    global_report['violations_found'] += space_report['violations_found']
            
            return global_report
        else:
            return self._get_space(space_id).validate_global_coherence()
    
    def get_spaces(self):
        """Retorna lista de espacios l√≥gicos disponibles."""
        return list(self.universes.keys())
    
    def __len__(self):
        """Retorna n√∫mero total de entradas en todos los espacios."""
        return sum(len(universe) for universe in self.universes.values())
    
    def __repr__(self):
        spaces_info = {sid: len(univ) for sid, univ in self.universes.items()}
        return f"KnowledgeBase(spaces={len(self.universes)}, entries_by_space={spaces_info})"

# ===============================================================================
# GENERADOR DE RUIDO ADVERSARIAL
# ===============================================================================

class AdversarialNoiseGenerator:
    """
    Generador de ruido adversarial para tests de robustez Aurora.
    Inyecta corrupci√≥n controlada para validar resistencia del sistema.
    """
    
    def __init__(self, corruption_ratio=0.1, seed=42):
        self.corruption_ratio = corruption_ratio
        self.seed = seed
        import random
        random.seed(seed)
    
    def corrupt_ms_patterns(self, clean_entries, corruption_types=None):
        """
        Inyecta corrupci√≥n en patrones Ms para test adversarial.
        
        Args:
            clean_entries: Lista de entradas KB limpias
            corruption_types: Tipos de corrupci√≥n ['flip_bits', 'inject_none', 'swap_values']
        
        Returns:
            Lista con entradas originales + entradas corruptas
        """
        import random
        import copy
        
        if corruption_types is None:
            corruption_types = ['flip_bits', 'inject_none', 'swap_values', 'corrupt_metam']
        
        corrupted_entries = []
        n_corrupt = int(len(clean_entries) * self.corruption_ratio)
        
        for i in range(n_corrupt):
            # Seleccionar entrada aleatoria para corromper
            base_entry = copy.deepcopy(random.choice(clean_entries))
            corruption_type = random.choice(corruption_types)
            
            if corruption_type == 'flip_bits':
                # Flipear bits aleatorios en Ms
                ms = base_entry['M_emergent']
                bit_to_flip = random.randint(0, 2)
                if ms[bit_to_flip] is not None:
                    ms[bit_to_flip] = 1 - ms[bit_to_flip]
                
            elif corruption_type == 'inject_none':
                # Inyectar None en posici√≥n aleatoria
                ms = base_entry['M_emergent'] 
                none_pos = random.randint(0, 2)
                ms[none_pos] = None
                
            elif corruption_type == 'swap_values':
                # Intercambiar valores en Ms
                ms = base_entry['M_emergent']
                if len(ms) >= 2:
                    i, j = random.sample(range(len(ms)), 2)
                    ms[i], ms[j] = ms[j], ms[i]
            
            elif corruption_type == 'corrupt_metam':
                # üîß FIX: Nueva corrupci√≥n en MetaM para test completo
                metam = base_entry['MetaM']
                corruption_pos = random.randint(0, 2)
                if metam[corruption_pos] is not None:
                    metam[corruption_pos] = 1 - metam[corruption_pos]  # Flip bit
            
            # Marcar como entrada corrupta
            base_entry['is_corrupted'] = True
            base_entry['corruption_type'] = corruption_type
            corrupted_entries.append(base_entry)
        
        # Combinar entradas limpias + corruptas
        all_entries = clean_entries + corrupted_entries
        random.shuffle(all_entries)  # Mezclar para test ciego
        
        return all_entries, len(corrupted_entries)
    
    def test_outlier_detection(self, kb, evolver, corrupted_entries):
        """
        Testa la capacidad del Evolver de detectar y descartar outliers.
        Simula votaci√≥n ponderada con entradas corruptas.
        """
        results = {
            'total_tested': len(corrupted_entries),
            'outliers_detected': 0,
            'false_positives': 0,
            'detection_accuracy': 0.0
        }
        
        # Extraer Ms de todas las entradas
        all_ms = [entry['M_emergent'] for entry in corrupted_entries]
        clean_ms = [entry['M_emergent'] for entry in corrupted_entries 
                   if not entry.get('is_corrupted', False)]
        
        # An√°lisis relacional para detectar outliers
        if len(all_ms) >= 3:
            relational_analysis = evolver.relate_vectors(all_ms)
            detected_outliers = relational_analysis.get('outliers', [])
            
            # Verificar precisi√≥n de detecci√≥n
            for outlier_idx in detected_outliers:
                entry = corrupted_entries[outlier_idx]
                if entry.get('is_corrupted', False):
                    results['outliers_detected'] += 1
                else:
                    results['false_positives'] += 1
            
            # Calcular accuracy
            if results['total_tested'] > 0:
                results['detection_accuracy'] = results['outliers_detected'] / results['total_tested']
        
        return results

# ===============================================================================
# FUNCIONES DE UTILIDAD Y TESTING
# ===============================================================================

def test_aurora_system():
    """
    Test b√°sico de integraci√≥n del sistema Aurora Trinity-3.
    Verifica que todos los componentes trabajen en armon√≠a.
    """
    print("[TEST] Iniciando test de integraci√≥n Aurora Trinity-3...")
    
    # Crear componentes del sistema
    kb = KnowledgeBase()
    evolver = Evolver()
    extender = Extender(kb, evolver)
    transcender = Transcender()
    
    # Test 1: Tensor fractal y s√≠ntesis
    print("[TEST] 1. Generando tensor fractal y procesando...")
    ft = FractalTensor.random()
    results = transcender.compute_fractal(ft)
    
    assert 'nivel_3' in results, "Fallo en s√≠ntesis fractal"
    print("[PASS] S√≠ntesis fractal completada")
    
    # Test 2: Almacenamiento en KB con validaci√≥n de coherencia
    print("[TEST] 2. Validando almacenamiento y coherencia...")
    if results['nivel_3']:
        entry = results['nivel_3'][0]
        Ms = entry['M_emergent']
        MetaM = entry['MetaM']
        
        # Verificar que no hay None en Ms/MetaM antes de almacenar
        if Ms and all(x is not None for x in Ms) and MetaM and all(x is not None for x in MetaM):
            kb.add_entry(
                A=entry['A'], B=entry['B'], C=entry['C'],
                M_emergent=Ms, MetaM=MetaM,
                R_validos=[[Ms[0], Ms[1], None]]  # Patr√≥n Ss
            )
            print("[PASS] Entrada almacenada sin violaciones de coherencia")
        else:
            print("[SKIP] Entrada contiene None, omitiendo por Honestidad Computacional")
    
    # Test 3: B√∫squeda y reconstrucci√≥n
    print("[TEST] 3. Probando b√∫squeda y reconstrucci√≥n...")
    if len(kb) > 0:
        # Buscar entrada reci√©n almacenada
        sample_entry = kb.all_entries()[0]
        sample_ms = sample_entry['M_emergent']
        
        # B√∫squeda exacta
        found = kb.find_by_ms(sample_ms, radius=0)
        assert len(found) > 0, "Fallo en b√∫squeda exacta"
        
        # B√∫squeda difusa
        found_fuzzy = kb.find_by_ms(sample_ms, radius=1)
        assert len(found_fuzzy) >= len(found), "Fallo en b√∫squeda difusa"
        
        print(f"[PASS] B√∫squeda: {len(found)} exactas, {len(found_fuzzy)} difusas")
    
    # Test 4: An√°lisis de Evolver
    print("[TEST] 4. Probando an√°lisis de Evolver...")
    test_metams = [[1,0,1], [0,1,0], [1,1,0]]
    analysis = evolver.analyze_metaMs(test_metams)
    assert 'coherencia_logica' in analysis, "Fallo en an√°lisis de MetaMs"
    
    axiom = evolver.formalize_axiom(analysis, test_metams)
    assert isinstance(axiom, str), "Fallo en formalizaci√≥n de axiomas"
    print("[PASS] An√°lisis de Evolver completado")
    
    # Test 5: Extender con reconstrucci√≥n
    print("[TEST] 5. Probando Extender con reconstrucci√≥n...")
    test_ss = [1, 0, None]  # Patr√≥n t√≠pico para reconstruir
    rebuilt = extender.rebuild([1,0,1], [0,1,0], test_ss)
    assert rebuilt is not None, "Fallo en reconstrucci√≥n"
    assert len(rebuilt) == 3, "Reconstrucci√≥n incompleta"
    print(f"[PASS] Reconstrucci√≥n: {test_ss} -> {rebuilt}")
    
    # Test 6: Validaci√≥n global de coherencia
    print("[TEST] 6. Validando coherencia global del sistema...")
    coherence_report = kb.validate_global_coherence()
    assert coherence_report['is_coherent'], f"Violaciones de coherencia: {coherence_report['violations_found']}"

    stats = kb.get_coherence_stats()
    print(f"[PASS] Coherencia global: {stats['coherence_ratio']:.3f}")
    
    print("[SUCCESS] Todos los tests pasaron - Sistema Aurora operativo")
    return True

def make_sequence(base: int, delta: int, progression_type: str = "arithmetic"):
    """
    üîß ENHANCED: Generador de secuencias aritm√©tica, geom√©trica y mixta.
    
    Args:
        base: Valor inicial
        delta: Incremento (aritm√©tica) o raz√≥n-1 (geom√©trica)
        progression_type: "arithmetic", "geometric", "mixed"
    
    Returns:
        tuple: (trio, delta_info, target, metadata)
    """
    if progression_type == "geometric":
        # Progresi√≥n geom√©trica: base, base*r, base*r¬≤
        ratio = delta + 1  # delta=1 -> ratio=2, delta=2 -> ratio=3
        trio = [base, base * ratio, base * (ratio ** 2)]
        target = base * (ratio ** 3)
        metadata = {"tipo": "geometrico", "ratio": ratio, "base": base}
        return trio, ratio, target, metadata
    
    elif progression_type == "mixed":
        # Alternar entre aritm√©tica y geom√©trica
        import random
        if random.choice([True, False]):
            return make_sequence(base, delta, "arithmetic")
        else:
            return make_sequence(base, min(delta, 2), "geometric")  # Limitar ratio
    
    else:  # arithmetic (default)
        trio = [base, base + delta, base + 2 * delta]
        target = base + 3 * delta
        metadata = {"tipo": "aritmetico", "delta": delta, "base": base}
        return trio, delta, target, metadata

def generate_mixed_dataset(n_arithmetic: int = 25, n_geometric: int = 25):
    """
    üîß NUEVO: Genera dataset mixto para evaluar separaci√≥n de contextos.
    
    Returns:
        list: Lista de casos con metadatos de tipo de progresi√≥n
    """
    import random
    dataset = []
    
    # Casos aritm√©ticos
    for _ in range(n_arithmetic):
        base = random.randint(10, 50)
        delta = random.choice([1, 2, 3, 4])
        trio, delta_val, target, meta = make_sequence(base, delta, "arithmetic")
        dataset.append({
            "secuencia": trio,
            "delta_esperado": delta_val,
            "valor_esperado": target,
            "patron_tipo": "aritmetico",
            "metadata": meta
        })
    
    # Casos geom√©tricos
    for _ in range(n_geometric):
        base = random.randint(2, 10)
        ratio_minus_1 = random.choice([1, 2])  # ratio ser√° 2 o 3
        trio, ratio, target, meta = make_sequence(base, ratio_minus_1, "geometric")
        dataset.append({
            "secuencia": trio,
            "razon": ratio,
            "valor_esperado": target,
            "patron_tipo": "geometrico",
            "metadata": meta
        })
    
    random.shuffle(dataset)  # Mezclar para evaluaci√≥n ciega
    return dataset

def evaluate_extender_enhanced(extender, test_cases: list):
    """
    üîß NUEVO: Evaluaci√≥n mejorada que distingue tipos de progresi√≥n.
    
    Args:
        extender: Instancia de Extender para evaluaci√≥n
        test_cases: Lista de casos generados por generate_mixed_dataset()
    
    Returns:
        dict: M√©tricas detalladas por tipo de progresi√≥n
    """
    results = {
        "aritmetico": {"hits": 0, "total": 0, "accuracy": 0.0},
        "geometrico": {"hits": 0, "total": 0, "accuracy": 0.0},
        "global": {"hits": 0, "total": 0, "accuracy": 0.0}
    }
    
    for case in test_cases:
        seq = case["secuencia"]
        pattern_type = case["patron_tipo"]
        expected_value = case["valor_esperado"]
        
        # Crear secuencia incompleta para reconstrucci√≥n
        # Simular patr√≥n [Œî‚ÇÅ, ŒîŒî, None]
        if len(seq) >= 2:
            delta_1 = seq[1] - seq[0]
            delta_delta = seq[2] - seq[1] - delta_1 if len(seq) > 2 else delta_1
            ss_pattern = [delta_1, delta_delta, None]
            
            # Intentar completar con el Extender Aurora
            rebuilt = extender.rebuild(None, None, ss_pattern)
            
            if rebuilt and len(rebuilt) >= 3:
                predicted_delta_2 = rebuilt[2]
                # Reconstruir valor esperado: seq[0] + rebuilt[0] + rebuilt[1] + rebuilt[2]
                predicted_value = seq[0] + sum(rebuilt)
                
                # Validaci√≥n con tolerancia para progresiones geom√©tricas
                tolerance = 1 if pattern_type == "aritmetico" else 2
                is_correct = abs(predicted_value - expected_value) <= tolerance
                
                if is_correct:
                    results[pattern_type]["hits"] += 1
                    results["global"]["hits"] += 1
        
        results[pattern_type]["total"] += 1
        results["global"]["total"] += 1
    
    # Calcular accuracy por categor√≠a
    for category in results:
        if results[category]["total"] > 0:
            results[category]["accuracy"] = results[category]["hits"] / results[category]["total"]
    
    return results

def test_enhanced_extender():
    """
    Test function to validate enhanced Extender functionality
    """
    print("[TEST] Testing Enhanced Extender with mixed progressions...")
    
    # Create enhanced Extender
    kb = KnowledgeBase()
    evolver = Evolver()
    extender = Extender(kb, evolver)
    
    # Generate mixed test dataset
    test_cases = generate_mixed_dataset(n_arithmetic=10, n_geometric=10)
    
    # Evaluate enhanced performance
    results = evaluate_extender_enhanced(extender, test_cases)
    
    print(f"üìä Enhanced Extender Results:")
    print(f"  Arithmetic  : {results['aritmetico']['accuracy']:.3f} "
          f"({results['aritmetico']['hits']}/{results['aritmetico']['total']})")
    print(f"  Geometric   : {results['geometrico']['accuracy']:.3f} "
          f"({results['geometrico']['hits']}/{results['geometrico']['total']})")
    print(f"  Global      : {results['global']['accuracy']:.3f}")
    
    # Test specific geometric sequence
    geometric_test = extender.rebuild(None, None, [2, 2, None])  # [2,4,8] pattern
    print(f"[TEST] Geometric [2,4,8] -> delta reconstruction: {geometric_test}")
    
    # Test specific arithmetic sequence  
    arithmetic_test = extender.rebuild(None, None, [1, 0, None])  # [1,2,3] pattern
    print(f"[TEST] Arithmetic [1,2,3] -> delta reconstruction: {arithmetic_test}")
    
    return results

if __name__ == "__main__":
    # Ejecutar test de integraci√≥n
    test_aurora_system()
    
    # Run enhanced Extender tests
    print("\n" + "="*60)
    print("TESTING ENHANCED EXTENDER")
    print("="*60)
    test_enhanced_extender()
    
    print("\n" + "="*80)
    print("AURORA TRINITY-3 - SISTEMA COMPLETAMENTE OPERATIVO")
    print("Arquitectura can√≥nica implementada con:")
    print("‚úÖ L√≥gica ternaria completa con Honestidad Computacional")
    print("‚úÖ LUTs optimizadas para 3^3 = 27 combinaciones")
    print("‚úÖ Principio de Coherencia Absoluta en KnowledgeBase")
    print("‚úÖ Evolver unificado (Arquetipo + Din√°mica + Relator")
    print("‚úÖ Extender arquitectural con reconstrucci√≥n inteligente")
    print("‚úÖ S√≠ntesis fractal jer√°rquica 27‚Üí9‚Üí3")
    print("‚úÖ Optimizaci√≥n con Ciclo Aureano (ARC)")
    print("="*80)
    
    print("\n" + "="*80)
    print("AURORA TRINITY-3 - SISTEMA COMPLETAMENTE OPERATIVO")
    print("Arquitectura can√≥nica implementada con:")
    print("‚úÖ L√≥gica ternaria completa con Honestidad Computacional")
    print("‚úÖ LUTs optimizadas para 3^3 = 27 combinaciones")
    print("‚úÖ Principio de Coherencia Absoluta en KnowledgeBase")
    print("‚úÖ Evolver unificado (Arquetipo + Din√°mica + Relator")
    print("‚úÖ Extender arquitectural con reconstrucci√≥n inteligente")
    print("‚úÖ S√≠ntesis fractal jer√°rquica 27‚Üí9‚Üí3")
    print("‚úÖ Optimizaci√≥n con Ciclo Aureano (ARC)")
    print("="*80)
