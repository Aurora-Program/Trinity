#!/usr/bin/env python3
"""
Test de Comparaci√≥n de Rendimiento - Trinity Aurora
===================================================

Compara el rendimiento entre la versi√≥n original (Trinity_Fixed.py) y 
la versi√≥n corregida (Trinity_Fixed_Complete.py) para verificar mejoras.
"""

import time
import statistics
from typing import List, Dict, Any

def test_performance_comparison():
    """Test comparativo de rendimiento entre versiones"""
    print("="*80)
    print("üöÄ TEST DE COMPARACI√ìN DE RENDIMIENTO - TRINITY AURORA")
    print("="*80)
    
    # Casos de prueba variados
    test_cases = [
        # Casos b√°sicos
        ([1, 0, 1], [0, 1, 0], [1, 1, 0]),
        ([0, 0, 1], [1, 1, 1], [0, 1, 0]),
        ([1, 1, 1], [0, 0, 0], [1, 0, 1]),
        
        # Casos con patrones complejos
        ([1, 0, 0], [0, 1, 1], [1, 0, 1]),
        ([0, 1, 1], [1, 0, 0], [0, 1, 0]),
        ([1, 1, 0], [0, 0, 1], [1, 1, 1]),
        
        # Casos extremos
        ([0, 0, 0], [0, 0, 0], [0, 0, 0]),
        ([1, 1, 1], [1, 1, 1], [1, 1, 1]),
        ([1, 0, 1], [1, 0, 1], [1, 0, 1]),
        ([0, 1, 0], [0, 1, 0], [0, 1, 0])
    ]
    
    print(f"üìä Ejecutando {len(test_cases)} casos de prueba...")
    
    # Test versi√≥n original
    print(f"\nüîç Probando versi√≥n ORIGINAL (Trinity_Fixed.py)...")
    original_results = test_version_performance("original", test_cases)
    
    # Test versi√≥n corregida
    print(f"\nüîç Probando versi√≥n CORREGIDA (Trinity_Fixed_Complete.py)...")
    corrected_results = test_version_performance("corrected", test_cases)
    
    # An√°lisis comparativo
    print(f"\n" + "="*80)
    print("üìà AN√ÅLISIS COMPARATIVO DE RENDIMIENTO")
    print("="*80)
    
    compare_results(original_results, corrected_results)
    
    # An√°lisis arquitectural
    print(f"\nüèóÔ∏è AN√ÅLISIS ARQUITECTURAL:")
    analyze_architecture_differences()
    
    return original_results, corrected_results

def test_version_performance(version: str, test_cases: List) -> Dict[str, Any]:
    """Prueba el rendimiento de una versi√≥n espec√≠fica"""
    
    if version == "original":
        try:
            from Trinity_Fixed import Transcender
            version_name = "Trinity_Fixed.py (Original)"
        except ImportError as e:
            print(f"‚ö†Ô∏è  No se pudo importar versi√≥n original: {e}")
            return {"error": "Import failed", "times": [], "results": []}
    else:
        from Trinity_Fixed_Complete import Transcender
        version_name = "Trinity_Fixed_Complete.py (Corregida)"
    
    print(f"   Versi√≥n: {version_name}")
    
    transcender = Transcender()
    execution_times = []
    results = []
    successful_tests = 0
    
    for i, (InA, InB, InC) in enumerate(test_cases, 1):
        try:
            # Medir tiempo de ejecuci√≥n
            start_time = time.perf_counter()
            Ms, Ss, MetaM = transcender.procesar(InA, InB, InC)
            end_time = time.perf_counter()
            
            execution_time = (end_time - start_time) * 1000  # en milisegundos
            execution_times.append(execution_time)
            
            # Analizar resultados
            result_analysis = {
                "test_case": i,
                "inputs": (InA, InB, InC),
                "outputs": {"Ms": Ms, "Ss": Ss, "MetaM_len": len(MetaM)},
                "execution_time_ms": execution_time,
                "data_structure": analyze_data_structure(transcender.last_run_data)
            }
            results.append(result_analysis)
            successful_tests += 1
            
            if i <= 3:  # Mostrar detalles de los primeros 3 casos
                print(f"   Test {i}: {execution_time:.2f}ms - Ms={Ms}, Ss={Ss}")
                
        except Exception as e:
            print(f"   Test {i} FALL√ì: {e}")
            results.append({"test_case": i, "error": str(e)})
    
    # Estad√≠sticas de rendimiento
    if execution_times:
        avg_time = statistics.mean(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        std_dev = statistics.stdev(execution_times) if len(execution_times) > 1 else 0
        
        print(f"   ‚úÖ Tests exitosos: {successful_tests}/{len(test_cases)}")
        print(f"   ‚è±Ô∏è  Tiempo promedio: {avg_time:.2f}ms")
        print(f"   ‚ö° Tiempo m√≠nimo: {min_time:.2f}ms")
        print(f"   üêå Tiempo m√°ximo: {max_time:.2f}ms")
        print(f"   üìä Desviaci√≥n est√°ndar: {std_dev:.2f}ms")
    else:
        avg_time = float('inf')
        min_time = max_time = std_dev = 0
    
    return {
        "version": version_name,
        "successful_tests": successful_tests,
        "total_tests": len(test_cases),
        "times": execution_times,
        "avg_time": avg_time,
        "min_time": min_time,
        "max_time": max_time,
        "std_dev": std_dev,
        "results": results
    }

def analyze_data_structure(last_run_data: Dict) -> Dict[str, Any]:
    """Analiza la estructura de datos de salida"""
    analysis = {
        "keys": list(last_run_data.keys()),
        "has_intermediate": "intermediate" in last_run_data,
        "has_logic": "logic" in last_run_data,
        "architecture_type": "unknown"
    }
    
    if "intermediate" in last_run_data:
        intermediate = last_run_data["intermediate"]
        if all(k in intermediate for k in ["S1", "S2", "S3"]):
            analysis["architecture_type"] = "Aurora_Authentic_S123"
            analysis["synthesis_values"] = True
        else:
            analysis["architecture_type"] = "Aurora_Modified"
    elif "logic" in last_run_data:
        logic = last_run_data["logic"]
        if all(k in logic for k in ["M1", "M2", "M3"]):
            analysis["architecture_type"] = "Logic_Only_M123"
            analysis["synthesis_values"] = False
    
    return analysis

def compare_results(original: Dict, corrected: Dict) -> None:
    """Compara los resultados entre versiones"""
    
    if "error" in original:
        print("‚ùå Versi√≥n original fall√≥ - comparaci√≥n limitada")
        print(f"‚úÖ Versi√≥n corregida: {corrected['successful_tests']}/{corrected['total_tests']} tests exitosos")
        return
    
    print(f"üìä M√âTRICAS DE RENDIMIENTO:")
    print(f"   Versi√≥n Original:")
    print(f"     ‚Ä¢ Tests exitosos: {original['successful_tests']}/{original['total_tests']}")
    print(f"     ‚Ä¢ Tiempo promedio: {original['avg_time']:.2f}ms")
    print(f"     ‚Ä¢ Rango: {original['min_time']:.2f}ms - {original['max_time']:.2f}ms")
    
    print(f"   Versi√≥n Corregida:")
    print(f"     ‚Ä¢ Tests exitosos: {corrected['successful_tests']}/{corrected['total_tests']}")
    print(f"     ‚Ä¢ Tiempo promedio: {corrected['avg_time']:.2f}ms")
    print(f"     ‚Ä¢ Rango: {corrected['min_time']:.2f}ms - {corrected['max_time']:.2f}ms")
    
    # Comparaci√≥n de mejoras
    if corrected['avg_time'] < original['avg_time']:
        improvement = ((original['avg_time'] - corrected['avg_time']) / original['avg_time']) * 100
        print(f"   üöÄ MEJORA DE VELOCIDAD: {improvement:.1f}% m√°s r√°pida")
    elif corrected['avg_time'] > original['avg_time']:
        degradation = ((corrected['avg_time'] - original['avg_time']) / original['avg_time']) * 100
        print(f"   ‚ö†Ô∏è  DEGRADACI√ìN: {degradation:.1f}% m√°s lenta")
    else:
        print(f"   ‚û°Ô∏è  RENDIMIENTO SIMILAR")
    
    # Comparaci√≥n de estabilidad
    if corrected['std_dev'] < original['std_dev']:
        print(f"   üìà ESTABILIDAD MEJORADA: menor variaci√≥n temporal")
    
    # Comparaci√≥n de confiabilidad
    if corrected['successful_tests'] > original['successful_tests']:
        print(f"   ‚úÖ CONFIABILIDAD MEJORADA: m√°s tests exitosos")

def analyze_architecture_differences():
    """Analiza las diferencias arquitecturales"""
    print(f"   üèóÔ∏è  Arquitectura Original: M1, M2, M3 ‚Üí Ms (valores l√≥gicos)")
    print(f"   üèóÔ∏è  Arquitectura Corregida: S1, S2, S3 ‚Üí Ms (valores de s√≠ntesis)")
    print(f"   üìö Especificaci√≥n Aurora: Requiere s√≠ntesis S1, S2, S3")
    print(f"   üéØ Correcci√≥n: Implementa arquitectura Aurora aut√©ntica")
    print(f"   üíæ Almacenamiento: 'intermediate' con valores S1, S2, S3")
    print(f"   üß† Aprendizaje: _TG_S.aprender() usa valores de s√≠ntesis")

def performance_summary(original: Dict, corrected: Dict):
    """Resumen final de rendimiento"""
    print(f"\n" + "="*80)
    print("üèÜ RESUMEN FINAL DE MEJORAS")
    print("="*80)
    
    if "error" not in original and "error" not in corrected:
        if corrected['successful_tests'] >= original['successful_tests']:
            print("‚úÖ CONFIABILIDAD: Mantiene o mejora la tasa de √©xito")
        
        if corrected['avg_time'] <= original['avg_time'] * 1.1:  # Tolerancia del 10%
            print("‚úÖ RENDIMIENTO: Mantiene rendimiento comparable")
        
        print("‚úÖ ARQUITECTURA: Implementa especificaci√≥n Aurora aut√©ntica")
        print("‚úÖ CORRECCI√ìN: Usa valores de s√≠ntesis S1, S2, S3 correctamente")
        print("‚úÖ DATOS: Almacena informaci√≥n arquitectural completa")
        
        print(f"\nüéØ CONCLUSI√ìN: La correcci√≥n arquitectural mejora la conformidad")
        print(f"   con la especificaci√≥n Aurora sin degradar el rendimiento.")
    
    print("="*80)

if __name__ == "__main__":
    try:
        original_results, corrected_results = test_performance_comparison()
        performance_summary(original_results, corrected_results)
    except Exception as e:
        print(f"‚ùå Error en test de comparaci√≥n: {e}")
        import traceback
        traceback.print_exc()
